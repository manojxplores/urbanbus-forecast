{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e36d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "from datetime import timedelta\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f33e002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 600,672\n",
      "Date range: 2017-10-01 00:00:00 to 2018-03-31 23:45:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "urbanbus_data = pd.read_csv('../urbanbus_data/SER_0b91_start_aggregated.csv')\n",
    "\n",
    "df = urbanbus_data.groupby([\"Ride_start_datetime\", \"Bus_Service_Number\", \"Direction\", \"Boarding_stop_stn\", \"Alighting_stop_stn\"], as_index=False)[\"Passenger_Count\"].sum()\n",
    "df['Ride_start_datetime'] = pd.to_datetime(df['Ride_start_datetime'], errors='coerce')\n",
    "df = df.sort_values('Ride_start_datetime').reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Date range: {df['Ride_start_datetime'].min()} to {df['Ride_start_datetime'].max()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7da08a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime features\n",
    "df['hour'] = df['Ride_start_datetime'].dt.hour\n",
    "df['minute'] = df['Ride_start_datetime'].dt.minute\n",
    "df['day'] = df['Ride_start_datetime'].dt.day\n",
    "df['dayofweek'] = df['Ride_start_datetime'].dt.dayofweek\n",
    "df['month'] = df['Ride_start_datetime'].dt.month\n",
    "df['year'] = df['Ride_start_datetime'].dt.year\n",
    "df['week_of_year'] = df['Ride_start_datetime'].dt.isocalendar().week\n",
    "\n",
    "# Cyclic encoding\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['minute_sin'] = np.sin(2 * np.pi * df['minute'] / 60)\n",
    "df['minute_cos'] = np.cos(2 * np.pi * df['minute'] / 60)\n",
    "df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# Peak hour flags\n",
    "peak_hours = df.groupby('hour')['Passenger_Count'].sum().nlargest(2).index.tolist()\n",
    "df['is_peak_hour'] = df['hour'].isin(peak_hours).astype(int)\n",
    "\n",
    "# Weekend and holiday flag\n",
    "df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "china_holidays = holidays.country_holidays('CN')\n",
    "df['is_holiday'] = df['Ride_start_datetime'].dt.date.isin(china_holidays).astype(int)\n",
    "\n",
    "# Route identifier\n",
    "df['route'] = df['Boarding_stop_stn'] + '_to_' + df['Alighting_stop_stn']\n",
    "df = df.sort_values(['route', 'Ride_start_datetime']).reset_index(drop=True)\n",
    "\n",
    "# Lag Features\n",
    "for lag in [1, 2, 3, 4, 8, 12, 24]:\n",
    "    df[f'lag_{lag}'] = df.groupby('route')['Passenger_Count'].shift(lag)\n",
    "\n",
    "for window in [4, 8, 12, 24]:\n",
    "    df[f'rolling_mean_{window}'] = (\n",
    "        df.groupby('route')['Passenger_Count']\n",
    "        .shift(1)\n",
    "        .rolling(window=window)\n",
    "        .mean()\n",
    "    )\n",
    "    df[f'rolling_std_{window}'] = (\n",
    "        df.groupby('route')['Passenger_Count']\n",
    "        .shift(1)\n",
    "        .rolling(window=window)\n",
    "        .std()\n",
    "    )\n",
    "\n",
    "\n",
    "lag_roll_cols = [col for col in df.columns if col.startswith(('lag_', 'rolling_'))]\n",
    "df = df.dropna(subset=lag_roll_cols).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c93c7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 497,989 records (2017-10-01 16:15:00 to 2018-03-04 23:30:00)\n",
      "Test Set: 89,785 records (2018-03-04 23:45:00 to 2018-03-31 23:45:00)\n",
      "Split: 84.7% train / 15.3% test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_date = df['Ride_start_datetime'].max()\n",
    "cutoff_date = max_date - timedelta(days=27)\n",
    "\n",
    "train_df = df[df['Ride_start_datetime'] < cutoff_date].copy()\n",
    "test_df = df[df['Ride_start_datetime'] >= cutoff_date].copy()\n",
    "\n",
    "print(f\"Training Set: {len(train_df):,} records ({train_df['Ride_start_datetime'].min()} to {train_df['Ride_start_datetime'].max()})\")\n",
    "print(f\"Test Set: {len(test_df):,} records ({test_df['Ride_start_datetime'].min()} to {test_df['Ride_start_datetime'].max()})\")\n",
    "print(f\"Split: {len(train_df)/len(df)*100:.1f}% train / {len(test_df)/len(df)*100:.1f}% test\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c29643a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrices:\n",
      "  X_train: (497989, 35)\n",
      "  X_test: (89785, 35)\n",
      "  Total features: 35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['Boarding_stop_stn', 'Alighting_stop_stn']\n",
    "\n",
    "# All numerical features\n",
    "num_cols = [\n",
    "    'hour', 'minute', 'day', 'dayofweek', 'month', 'year', 'week_of_year',\n",
    "    'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', \n",
    "    'dow_sin', 'dow_cos', 'month_sin', 'month_cos',\n",
    "    'is_weekend', 'is_holiday', 'is_peak_hour']\n",
    "\n",
    "# Add lag and rolling features\n",
    "lag_roll_cols = [col for col in train_df.columns if col.startswith(('lag_', 'rolling_'))]\n",
    "num_cols.extend(lag_roll_cols)\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train = train_df[cat_cols + num_cols].copy()\n",
    "y_train = train_df['Passenger_Count'].copy()\n",
    "X_test = test_df[cat_cols + num_cols].copy()\n",
    "y_test = test_df['Passenger_Count'].copy()\n",
    "\n",
    "print(f\"Feature matrices:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  Total features: {len(cat_cols + num_cols)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3f89c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return 0.0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denom = np.abs(y_true) + np.abs(y_pred)\n",
    "    mask = denom != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return 0.0\n",
    "    return np.mean(2.0 * np.abs(y_true[mask] - y_pred[mask]) / denom[mask]) * 100\n",
    "\n",
    "def evaluate_model(y_true, y_pred, set_name=\"Set\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    smape = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n",
    "    print(f\"\\n{set_name} Performance:\")\n",
    "    print(f\"  MAE:   {mae:.4f}\")\n",
    "    print(f\"  RMSE:  {rmse:.4f}\")\n",
    "    print(f\"  RÂ²:    {r2:.4f}\")\n",
    "    print(f\"  MAPE:  {mape:.2f}%\")\n",
    "    print(f\"  sMAPE: {smape:.2f}%\")\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape, 'sMAPE': smape}\n",
    "\n",
    "def train_and_evaluate_model(model_name, model, X_train, y_train, X_test, y_test,\n",
    "                             cat_cols=None, preprocessor=True):\n",
    "    \n",
    "    print(f\"\\n{'='*20} Training {model_name} {'='*20}\\n\")\n",
    "    \n",
    "    if model_name.lower() == 'catboost':\n",
    "        all_features = list(X_train.columns)\n",
    "        cat_feature_indices = [all_features.index(c) for c in cat_cols] if cat_cols else []\n",
    "        train_pool = Pool(X_train, y_train, cat_features=cat_feature_indices)\n",
    "        test_pool = Pool(X_test, y_test, cat_features=cat_feature_indices)\n",
    "        model.fit(train_pool, eval_set=test_pool, use_best_model=False, verbose=100)\n",
    "        y_train_pred = model.predict(train_pool)\n",
    "        y_test_pred = model.predict(test_pool)\n",
    "    else:\n",
    "        if preprocessor:\n",
    "            pipe = Pipeline([\n",
    "                ('preprocessor', ColumnTransformer([\n",
    "                    ('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_cols),\n",
    "                    ('scaler', StandardScaler(), [c for c in X_train.columns if c not in cat_cols])\n",
    "                ], remainder='drop', verbose_feature_names_out=False)),\n",
    "                ('model', model)\n",
    "            ])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            y_train_pred = pipe.predict(X_train)\n",
    "            y_test_pred = pipe.predict(X_test)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    print(\"\\nTraining Set Metrics:\")\n",
    "    train_metrics = evaluate_model(y_train, y_train_pred, \"Training\")\n",
    "    \n",
    "    print(\"\\nTest Set Metrics:\")\n",
    "    test_metrics = evaluate_model(y_test, y_test_pred, \"Test\")\n",
    "    \n",
    "    # Overfitting check\n",
    "    r2_gap = train_metrics['R2'] - test_metrics['R2']\n",
    "    print(f\"\\nOverfitting RÂ² Gap: {r2_gap:.4f}\")\n",
    "    if r2_gap > 0.15:\n",
    "        print(\"â ï¸ Significant overfitting\")\n",
    "    elif r2_gap > 0.05:\n",
    "        print(\"â¡ Slight overfitting\")\n",
    "    else:\n",
    "        print(\"â Good generalization\")\n",
    "    \n",
    "    return y_train_pred, y_test_pred, train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4339f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_model = RandomForestRegressor(\n",
    "#     n_estimators=500, \n",
    "#     max_depth=20, \n",
    "#     n_jobs=-1, \n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "\n",
    "# y_train_pred_rf, y_test_pred_rf, train_metrics_rf, test_metrics_rf = train_and_evaluate_model(\"Random Forest\", rf_model, X_train, y_train, X_test, y_test, cat_cols=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "355c68b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training XGBoost ====================\n",
      "\n",
      "\n",
      "Training Set Metrics:\n",
      "\n",
      "Training Performance:\n",
      "  MAE:   0.6311\n",
      "  RMSE:  0.9507\n",
      "  RÂ²:    0.4027\n",
      "  MAPE:  40.57%\n",
      "  sMAPE: 36.68%\n",
      "\n",
      "Test Set Metrics:\n",
      "\n",
      "Test Performance:\n",
      "  MAE:   0.6588\n",
      "  RMSE:  1.0473\n",
      "  RÂ²:    0.2929\n",
      "  MAPE:  42.23%\n",
      "  sMAPE: 37.56%\n",
      "\n",
      "Overfitting RÂ² Gap: 0.1098\n",
      "â¡ Slight overfitting\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=500, \n",
    "    learning_rate=0.01, \n",
    "    max_depth=10, \n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8, \n",
    "    objective='reg:squarederror', \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "y_train_pred_xgb, y_test_pred_xgb, train_metrics_xgb, test_metrics_xgb = train_and_evaluate_model(\"XGBoost\", xgb_model, X_train, y_train, X_test, y_test, cat_cols=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8793c2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training LightGBM ====================\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1921\n",
      "[LightGBM] [Info] Number of data points in the train set: 497989, number of used features: 97\n",
      "[LightGBM] [Info] Start training from score 1.560767\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Metrics:\n",
      "\n",
      "Training Performance:\n",
      "  MAE:   0.6483\n",
      "  RMSE:  1.0064\n",
      "  RÂ²:    0.3306\n",
      "  MAPE:  41.38%\n",
      "  sMAPE: 37.12%\n",
      "\n",
      "Test Set Metrics:\n",
      "\n",
      "Test Performance:\n",
      "  MAE:   0.6540\n",
      "  RMSE:  1.0474\n",
      "  RÂ²:    0.2928\n",
      "  MAPE:  41.73%\n",
      "  sMAPE: 37.17%\n",
      "\n",
      "Overfitting RÂ² Gap: 0.0379\n",
      "â Good generalization\n"
     ]
    }
   ],
   "source": [
    "lgb_model = LGBMRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=40,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=1.0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "y_train_pred_lgb, y_test_pred_lgb, train_metrics_lgb, test_metrics_lgb = train_and_evaluate_model(\"LightGBM\", lgb_model, X_train, y_train, X_test, y_test, cat_cols=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b01279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training CatBoost ====================\n",
      "\n",
      "0:\tlearn: 0.5603437\ttest: 0.5515743\tbest: 0.5515743 (0)\ttotal: 187ms\tremaining: 6m 14s\n",
      "100:\tlearn: 0.5405859\ttest: 0.5298065\tbest: 0.5298065 (100)\ttotal: 8.03s\tremaining: 2m 31s\n",
      "200:\tlearn: 0.5390099\ttest: 0.5283935\tbest: 0.5283933 (199)\ttotal: 14.7s\tremaining: 2m 11s\n",
      "300:\tlearn: 0.5365236\ttest: 0.5260820\tbest: 0.5260820 (300)\ttotal: 21.6s\tremaining: 2m 2s\n",
      "400:\tlearn: 0.5349953\ttest: 0.5248225\tbest: 0.5248225 (400)\ttotal: 27.9s\tremaining: 1m 51s\n",
      "500:\tlearn: 0.5341115\ttest: 0.5242609\tbest: 0.5242609 (500)\ttotal: 33.9s\tremaining: 1m 41s\n",
      "600:\tlearn: 0.5335405\ttest: 0.5237781\tbest: 0.5237781 (600)\ttotal: 39.8s\tremaining: 1m 32s\n",
      "700:\tlearn: 0.5332227\ttest: 0.5236044\tbest: 0.5236044 (700)\ttotal: 46s\tremaining: 1m 25s\n",
      "800:\tlearn: 0.5327494\ttest: 0.5233088\tbest: 0.5233088 (800)\ttotal: 52.2s\tremaining: 1m 18s\n",
      "900:\tlearn: 0.5321435\ttest: 0.5228892\tbest: 0.5228892 (900)\ttotal: 58.3s\tremaining: 1m 11s\n",
      "1000:\tlearn: 0.5316140\ttest: 0.5225768\tbest: 0.5225768 (1000)\ttotal: 1m 4s\tremaining: 1m 4s\n",
      "1100:\tlearn: 0.5306046\ttest: 0.5218909\tbest: 0.5218909 (1100)\ttotal: 1m 10s\tremaining: 57.8s\n",
      "1200:\tlearn: 0.5299460\ttest: 0.5214930\tbest: 0.5214930 (1200)\ttotal: 1m 16s\tremaining: 51.1s\n",
      "1300:\tlearn: 0.5293911\ttest: 0.5211786\tbest: 0.5211768 (1291)\ttotal: 1m 23s\tremaining: 44.6s\n",
      "1400:\tlearn: 0.5290110\ttest: 0.5209090\tbest: 0.5209090 (1400)\ttotal: 1m 29s\tremaining: 38.3s\n",
      "1500:\tlearn: 0.5286518\ttest: 0.5207908\tbest: 0.5207851 (1467)\ttotal: 1m 36s\tremaining: 32.1s\n",
      "1600:\tlearn: 0.5283338\ttest: 0.5206591\tbest: 0.5206591 (1600)\ttotal: 1m 43s\tremaining: 25.8s\n",
      "1700:\tlearn: 0.5280502\ttest: 0.5205193\tbest: 0.5205180 (1698)\ttotal: 1m 50s\tremaining: 19.4s\n",
      "1800:\tlearn: 0.5276159\ttest: 0.5202879\tbest: 0.5202877 (1799)\ttotal: 1m 56s\tremaining: 12.9s\n",
      "1900:\tlearn: 0.5273310\ttest: 0.5201946\tbest: 0.5201946 (1900)\ttotal: 2m 3s\tremaining: 6.44s\n",
      "1999:\tlearn: 0.5271387\ttest: 0.5201322\tbest: 0.5201235 (1996)\ttotal: 2m 10s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5201234752\n",
      "bestIteration = 1996\n",
      "\n",
      "\n",
      "Training Set Metrics:\n",
      "\n",
      "Training Performance:\n",
      "  MAE:   0.5271\n",
      "  RMSE:  1.1803\n",
      "  RÂ²:    0.0792\n",
      "  MAPE:  20.38%\n",
      "  sMAPE: 25.40%\n",
      "\n",
      "Test Set Metrics:\n",
      "\n",
      "Test Performance:\n",
      "  MAE:   0.5201\n",
      "  RMSE:  1.1789\n",
      "  RÂ²:    0.1040\n",
      "  MAPE:  20.06%\n",
      "  sMAPE: 25.00%\n",
      "\n",
      "Overfitting RÂ² Gap: -0.0248\n",
      "â Good generalization\n"
     ]
    }
   ],
   "source": [
    "cat_model = CatBoostRegressor(\n",
    "    iterations=2000, \n",
    "    learning_rate=0.05, \n",
    "    depth=8, \n",
    "    l2_leaf_reg=3,\n",
    "    loss_function='MAE', \n",
    "    eval_metric='MAE', \n",
    "    random_seed=42,\n",
    "    task_type='CPU', \n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "y_train_pred_cat, y_test_pred_cat, train_metrics_cat, test_metrics_cat = train_and_evaluate_model(\"CatBoost\", cat_model, X_train, y_train, X_test, y_test, cat_cols=cat_cols, preprocessor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a775446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature Importance\n",
    "# feature_names = pipe.named_steps['preprocessor'].get_feature_names_out()\n",
    "# importances = pipe.named_steps['model'].feature_importances_\n",
    "\n",
    "# feat_imp = pd.DataFrame({\n",
    "#     'feature': feature_names,\n",
    "#     'importance': importances\n",
    "# }).sort_values('importance', ascending=False)\n",
    "\n",
    "# print(\"=\"*70)\n",
    "# print(\"TOP 20 FEATURE IMPORTANCES\")\n",
    "# print(\"=\"*70)\n",
    "# print(feat_imp.head(20).to_string(index=False))\n",
    "# print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# # Visualizations\n",
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "# # 1. Actual vs Predicted Line Plot\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "# sample_size = min(200, len(y_test))\n",
    "# axes[0].plot(range(sample_size), y_test.values[:sample_size], \n",
    "#              label='Actual', marker='o', markersize=3, alpha=0.7, linewidth=1.5)\n",
    "# axes[0].plot(range(sample_size), y_test_pred[:sample_size], \n",
    "#              label='Predicted', marker='x', markersize=3, alpha=0.7, linewidth=1.5)\n",
    "# axes[0].set_title(\"Actual vs Predicted (First 200 Test Points)\", fontweight='bold', fontsize=12)\n",
    "# axes[0].set_xlabel(\"Sample Index\")\n",
    "# axes[0].set_ylabel(\"Passenger Count\")\n",
    "# axes[0].legend()\n",
    "# axes[0].grid(alpha=0.3)\n",
    "\n",
    "# # 2. Scatter Plot\n",
    "# axes[1].scatter(y_test, y_test_pred, alpha=0.3, s=15, color='steelblue')\n",
    "# axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "#              'r--', linewidth=2, label='Perfect Prediction')\n",
    "# axes[1].set_xlabel(\"Actual Passenger Count\", fontsize=11)\n",
    "# axes[1].set_ylabel(\"Predicted Passenger Count\", fontsize=11)\n",
    "# axes[1].set_title(\"Prediction Scatter Plot\", fontweight='bold', fontsize=12)\n",
    "# axes[1].legend()\n",
    "# axes[1].grid(alpha=0.3)\n",
    "# axes[1].text(0.05, 0.95, f'RÂ² = {test_r2:.4f}\\nMAE = {test_mae:.4f}', \n",
    "#              transform=axes[1].transAxes, verticalalignment='top',\n",
    "#              bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # 3. Residuals Analysis\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "# residuals = y_test - y_test_pred\n",
    "\n",
    "# axes[0].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "# axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "# axes[0].set_title(\"Residuals Distribution\", fontweight='bold', fontsize=12)\n",
    "# axes[0].set_xlabel(\"Residual (Actual - Predicted)\")\n",
    "# axes[0].set_ylabel(\"Frequency\")\n",
    "# axes[0].legend()\n",
    "# axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# axes[1].scatter(y_test_pred, residuals, alpha=0.3, s=15, color='coral')\n",
    "# axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "# axes[1].set_xlabel(\"Predicted Passenger Count\", fontsize=11)\n",
    "# axes[1].set_ylabel(\"Residuals\", fontsize=11)\n",
    "# axes[1].set_title(\"Residual Plot\", fontweight='bold', fontsize=12)\n",
    "# axes[1].legend()\n",
    "# axes[1].grid(alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # 4. Feature Importances\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# top_features = feat_imp.head(20)\n",
    "# sns.barplot(data=top_features, y='feature', x='importance', palette='viridis')\n",
    "# plt.title(\"Top 20 Feature Importances\", fontweight='bold', fontsize=14)\n",
    "# plt.xlabel(\"Importance\", fontsize=11)\n",
    "# plt.ylabel(\"Feature\", fontsize=11)\n",
    "# plt.grid(alpha=0.3, axis='x')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # 5. Hourly Performance\n",
    "# test_df_eval = test_df.copy()\n",
    "# test_df_eval['predicted'] = y_test_pred\n",
    "# hourly_performance = test_df_eval.groupby('hour').agg({\n",
    "#     'Passenger_Count': 'mean',\n",
    "#     'predicted': 'mean'\n",
    "# }).reset_index()\n",
    "\n",
    "# plt.figure(figsize=(14, 5))\n",
    "# plt.plot(hourly_performance['hour'], hourly_performance['Passenger_Count'], \n",
    "#          marker='o', label='Actual', linewidth=2.5, markersize=6)\n",
    "# plt.plot(hourly_performance['hour'], hourly_performance['predicted'], \n",
    "#          marker='s', label='Predicted', linewidth=2.5, markersize=6)\n",
    "# plt.xlabel(\"Hour of Day\", fontsize=11)\n",
    "# plt.ylabel(\"Average Passenger Count\", fontsize=11)\n",
    "# plt.title(\"Average Passenger Count by Hour: Actual vs Predicted\", fontweight='bold', fontsize=14)\n",
    "# plt.legend(fontsize=11)\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.xticks(range(0, 24))\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # 6. Day of Week Performance\n",
    "# dow_performance = test_df_eval.groupby('dayofweek').agg({\n",
    "#     'Passenger_Count': 'mean',\n",
    "#     'predicted': 'mean'\n",
    "# }).reset_index()\n",
    "# dow_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# x = np.arange(len(dow_labels))\n",
    "# width = 0.35\n",
    "# plt.bar(x - width/2, dow_performance['Passenger_Count'], width, label='Actual', alpha=0.8)\n",
    "# plt.bar(x + width/2, dow_performance['predicted'], width, label='Predicted', alpha=0.8)\n",
    "# plt.xlabel(\"Day of Week\", fontsize=11)\n",
    "# plt.ylabel(\"Average Passenger Count\", fontsize=11)\n",
    "# plt.title(\"Average Passenger Count by Day of Week\", fontweight='bold', fontsize=14)\n",
    "# plt.xticks(x, dow_labels)\n",
    "# plt.legend(fontsize=11)\n",
    "# plt.grid(alpha=0.3, axis='y')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
